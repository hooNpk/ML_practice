{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2016313844 Jeonghoon Park\n",
    "\n",
    "### Google Colab\n",
    "- Install and How to use it\n",
    "- Upload the data files on the Google Drive\n",
    "\n",
    "- Hard coding on MLP model for predicting MNIST data on Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "a='hello colab'\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist_train(60000개)과 mnist_test(10000개) 데이터를 각각 불러온다.\n",
    "data_file = open(\"/content/gdrive/My Drive/data/mnist_train.csv\", \"r\") #연결되어 있는 구글 드라이브에 업로드했던 데이터를 코랩 환경으로 불러오는 코드\n",
    "training_data = data_file.readlines()\n",
    "data_file.close()\n",
    "\n",
    "test_data_file = open(\"/content/gdrive/My Drive/data/mnist_test.csv\", \"r\")\n",
    "test_data = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib과 numpy라이브러리를 불러온 후 데이터 하나를 시각화해본다.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "t = np.asfarray(training_data[0].split(\",\"))\n",
    "\n",
    "# 일렬로 늘어진 픽셀정보를 28x28 행렬로 바꾼다\n",
    "n = t[1:].reshape(28,28)\n",
    "\n",
    "plt.imshow(n, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    #DeepNeuralNetwork 클래스를 initialize\n",
    "    def __init__(self, input_layers, hidden_layer_1, hidden_layer_2, hidden_layer_3, output_layers):\n",
    "        self.inputs = input_layers\n",
    "        self.hidden_1 = hidden_layer_1\n",
    "        self.hidden_2 = hidden_layer_2\n",
    "        self.hidden_3 = hidden_layer_3\n",
    "        self.outputs = output_layers\n",
    "        self.test_data = None\n",
    "\n",
    "        #가중치 값들을 모두 랜덤으로 초기화\n",
    "        self.w_ih = np.random.randn(self.inputs, self.hidden_1) / np.sqrt(self.inputs/2)\n",
    "        self.w_hh_12 = np.random.randn(self.hidden_1, self.hidden_2) / np.sqrt(self.hidden_1/2)\n",
    "        self.w_hh_23 = np.random.randn(self.hidden_2, self.hidden_3) / np.sqrt(self.hidden_2/2)\n",
    "        self.w_ho = np.random.randn(self.hidden_3, self.outputs) / np.sqrt(self.hidden_3/2)\n",
    "\n",
    "    # feed-forward를 진행한다.\n",
    "    def predict(self, x):\n",
    "        # 문자열을 float array로 바꾸는 과정\n",
    "        data = self.normalize(np.asfarray(x.split(',')))\n",
    "\n",
    "        # 0번은 레이블이므로 제외\n",
    "        data = data[1:]\n",
    "\n",
    "        #3개의 은닉층(2개의 sigmoid와 1개의 tanh)과 하나의 출력층(softmax)\n",
    "        layer_1 = self.sigmoid(np.dot(data, self.w_ih))\n",
    "        layer_2 = self.tanh(np.dot(layer_1, self.w_hh_12))\n",
    "        layer_3 = self.sigmoid(np.dot(layer_2, self.w_hh_23))\n",
    "        output = self.softmax(np.dot(layer_3, self.w_ho))\n",
    "        return output\n",
    "\n",
    "    # training_data로 학습 진행\n",
    "    def train(self, training_data, learning_rate, epoch):\n",
    "        for ech in range(0, epoch):\n",
    "            for i, x in enumerate(training_data):\n",
    "                target = np.array(np.zeros(self.outputs) + learning_rate, ndmin=2)\n",
    "                target[0][int(x[0])] = 1-learning_rate\n",
    "                x = self.normalize(np.asfarray(x.split(\",\")))\n",
    "\n",
    "                # feed-forward propagation\n",
    "                layer1 = self.sigmoid(np.dot(x[1:], self.w_ih))\n",
    "                layer2 = self.tanh(np.dot(layer1, self.w_hh_12))\n",
    "                layer3 = self.sigmoid(np.dot(layer2, self.w_hh_23))\n",
    "                layer4 = self.softmax(np.dot(layer3, self.w_ho))\n",
    "\n",
    "                # back propagation\n",
    "                layer4_reverse = (target - layer4)\n",
    "                layer3_reverse = layer4_reverse.dot(self.w_ho.T) * (layer3 * (1 - layer3))\n",
    "                layer2_reverse = layer3_reverse.dot(self.w_hh_23.T) * (1 - layer2) * (1 + layer2)\n",
    "                layer1_reverse = layer2_reverse.dot(self.w_hh_12.T) * (layer1 * (1 - layer1))\n",
    "\n",
    "                # weight update\n",
    "                self.w_ho = self.w_ho + learning_rate * layer4_reverse.T.dot(np.array(layer3, ndmin=2)).T\n",
    "                self.w_hh_23 = self.w_hh_23 + learning_rate * layer3_reverse.T.dot(np.array(layer2, ndmin=2)).T\n",
    "                self.w_hh_12 = self.w_hh_12 + learning_rate * layer2_reverse.T.dot(np.array(layer1, ndmin=2)).T\n",
    "                self.w_ih = self.w_ih + learning_rate * layer1_reverse.T.dot(np.array(x[1:], ndmin=2)).T\n",
    "\n",
    "                #2000개에 한 번씩 accuracy 출력\n",
    "                if i % 2000 == 0 :\n",
    "                    self.print_accuracy()\n",
    "\n",
    "    # 현재 neural network의 accuracy를 출력한다.\n",
    "    def print_accuracy(self):\n",
    "        matched = 0\n",
    "\n",
    "        for x in self.test_data:\n",
    "            label = int(x[0])\n",
    "            predicted = np.argmax(self.predict(x))\n",
    "            if label == predicted :\n",
    "                matched = matched + 1\n",
    "        print('accuracy : {0}'.format(matched/len(self.test_data)))\n",
    "\n",
    "    #sigmoid함수 정의\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "    #feature scaling을 위한 normalize 함수 정의\n",
    "    def normalize(self, x):\n",
    "        return (x / 255.0) * 0.99 + 0.01\n",
    "    \n",
    "    #tanh함수 정의\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "        \n",
    "    #softmax함수 정의\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input layer, hidden layer 1, 2, 3, output layer의 노드 수를 각각 784, 100, 100, 100, 10개로 설정\n",
    "network = DeepNeuralNetwork(784, 100, 100, 100, 10)\n",
    "network.test_data = test_data\n",
    "#learning rate은 0.01, epoch는 1로 설정\n",
    "network.train(training_data, 0.01, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
